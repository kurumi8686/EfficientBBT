---

original formula:
tuned_small_logits + a * (large_logits - original_small_logits)

if: 舍弃大模型logits？（思想：更换large_logits似乎对结果不造成什么影响，即使large_logits非常差）
公式变为：tuned_small_logits + a * (0 - original_small_logits)
训练阶段、inference阶段均使用上述公式

                   acc                   原始GP_filter的acc            stable GP_filter acc
COPA accuracy: 0.8760 (438/500)           0.9160 (458/500)             0.9200 (460/500)
CoLA accuracy: 0.8476 (884/1043)          0.8562 (893/1043)            0.8581 (895/1043)
MRPC accuracy: 0.8431 (344/408)           0.8676 (354/408)             0.8603 (351/408)
AGne accuracy: 0.9543 (7253/7600)         0.9522 (7237/7600)           0.9522 (7237/7600)
SST2 accuracy: 0.9656 (842/872)           0.9713 (847/872)             0.9713 (847/872)
ARCC accuracy: 0.5518 (165/299)           0.6087 (182/299)             0.6087 (182/299)
CsQA accuracy: 0.7928 (968/1221)          0.7895 (964/1221)            0.7928 (968/1221)
OBQA accuracy: 0.7840 (392/500)           0.8020 (401/500)             0.7980 (399/500)
QNLI accuracy: 0.9517 (5199/5463)         0.9498 (5189/5463)           0.9517 (5199/5463)
RTE  accuracy: 0.8051 (223/277)           0.8773 (243/277)             0.8773 (243/277)
avg.              0.8372                     0.8591                    0.8590
性能介于 7b-loratune 和 CPT/GP 方法之间

---

测试CPT框架代码的直接微调性能：
公式变为：tuned_small_logits
训练阶段、inference阶段均使用上述公式
本质相当于直接微调模型（预估性能比lora-tune高一些）

               7b direct-tune acc      7b-lora-tune acc          GP_filter acc
COPA accuracy: 0.9020 (451/500)        0.8160 (408/500)         0.9160 (458/500)
CoLA accuracy: 0.8485 (885/1043)       0.8428 (879/1043)        0.8562 (893/1043)
MRPC accuracy: 0.8529 (348/408)        0.6838 (279/408)         0.8676 (354/408)
AGne accuracy: 0.9551 (7259/7600)      0.9359 (7113/7600)       0.9522 (7237/7600)
SST2 accuracy: 0.9713 (847/872)        0.9576 (835/872)         0.9713 (847/872)
ARCC accuracy: 0.5351 (160/299)        0.4883 (146/299)         0.6087 (182/299)
CsQA accuracy: 0.7813 (954/1221)       0.7592 (927/1221)        0.7895 (964/1221)
OBQA accuracy: 0.7860 (393/500)        0.7560 (378/500)         0.8020 (401/500)
QNLI accuracy: 0.9493 (5186/5463)      0.8836 (4827/5463)       0.9498 (5189/5463)
RTE  accuracy: 0.8267 (229/277)        0.8303 (230/277)         0.8773 (243/277)
avg.               0.8408                  0.7954                    0.8591

---

训练阶段公式变为：tuned_small_logits + a * (gp_logits)
inference阶段公式变为：tuned_small_logits + a * (large_logits)

                    acc                GP_filter acc             CPT acc
COPA accuracy: 0.9080 (454/500)       0.9160 (458/500)        0.9060 (453/500)
CoLA accuracy: 0.8591 (896/1043)      0.8562 (893/1043)       0.8591 (896/1043)
MRPC accuracy: 0.8382 (342/408)       0.8676 (354/408)        0.8554 (349/408)
AGne accuracy: 0.9504 (7223/7600)     0.9522 (7237/7600)      0.9545 (7254/7600)
SST2 accuracy: 0.9702 (846/872)       0.9713 (847/872)        0.9702 (846/872)
ARCC accuracy: 0.6054 (181/299)       0.6087 (182/299)        0.5987 (179/299)
CsQA accuracy: 0.7797 (952/1221)      0.7895 (964/1221)       0.7789 (951/1221)
OBQA accuracy: 0.8020 (401/500)       0.8020 (401/500)        0.7980 (399/500)
QNLI accuracy: 0.9506 (5193/5463)     0.9498 (5189/5463)      0.9528 (5205/5463)
RTE  accuracy: 0.8628 (239/277)       0.8773 (243/277)        0.8592 (238/277)
avg.              0.8526                  0.8591                  0.8533

其实和原本的GP方法效果相当，主要是MRPC低了一点。是否说明 "-original_small_logits" 不重要？


